{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded72692-f3da-4d3f-9ae8-4c0f28f03c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7b81f0-1a6f-4df8-972b-d624717b72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader , Subset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a6d0ac-6a68-4dac-a843-951d50e2c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b51bcd0-7e36-4d66-a92e-2601d335e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "030a3e87-bb77-4400-b9ec-843780d266af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1794dcdf-3606-44c6-bb5d-e7a337093f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from preprocessor.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "174dd7f6-9549-4e47-873a-2f03b4e67481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1481942a-651e-4b42-9873-37e4a2c2db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef754e-fc52-4e03-8a53-94daa4011d1f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d3f3baa-b417-4428-b783-c324df79b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '../../../Data/'\n",
    "data = pd.read_excel(dir_path + '한국어_대화체_번역.xlsx' , engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e9ea561-b5b6-4c53-b121-cc2ff56aa337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>원문</th>\n",
       "      <th>번역문</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이번 신제품 출시에 대한 시장의 반응은 어떤가요?</td>\n",
       "      <td>How is the market's reaction to the newly rele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>판매량이 지난번 제품보다 빠르게 늘고 있습니다.</td>\n",
       "      <td>The sales increase is faster than the previous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그렇다면 공장에 연락해서 주문량을 더 늘려야겠네요.</td>\n",
       "      <td>Then, we'll have to call the manufacturer and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>네, 제가 연락해서 주문량을 2배로 늘리겠습니다.</td>\n",
       "      <td>Sure, I'll make a call and double the volume o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>지난 회의 마지막에 논의했던 안건을 다시 볼까요?</td>\n",
       "      <td>Shall we take a look at the issues we discusse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             원문  \\\n",
       "0   이번 신제품 출시에 대한 시장의 반응은 어떤가요?   \n",
       "1    판매량이 지난번 제품보다 빠르게 늘고 있습니다.   \n",
       "2  그렇다면 공장에 연락해서 주문량을 더 늘려야겠네요.   \n",
       "3   네, 제가 연락해서 주문량을 2배로 늘리겠습니다.   \n",
       "4   지난 회의 마지막에 논의했던 안건을 다시 볼까요?   \n",
       "\n",
       "                                                 번역문  \n",
       "0  How is the market's reaction to the newly rele...  \n",
       "1  The sales increase is faster than the previous...  \n",
       "2  Then, we'll have to call the manufacturer and ...  \n",
       "3  Sure, I'll make a call and double the volume o...  \n",
       "4  Shall we take a look at the issues we discusse...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['원문','번역문']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fe14d6c-1b6b-4094-972e-d5afa9b25ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = data['번역문']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af8f1688-bf9a-412c-aca4-99a44a410e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = preprocessor.VocabSet(word_tokenize)\n",
    "en_tokens = vs.tokens(en_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab373bf7-2fed-4c00-aafa-5b4b588ff6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_encoder = preprocessor.Encoder(en_data , word_tokenize , en_tokens)\n",
    "en_encoded = en_encoder.encode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a404536-64ec-4e47-9e3a-bcfbfc78c8b5",
   "metadata": {},
   "source": [
    "## Context Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1f303f6-6ef2-4e81-a634-c68dd178820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_size = 11\n",
    "\n",
    "en_word2vec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84d8b5d7-30d4-4948-823f-32c73b2da840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 74237.27it/s]\n"
     ]
    }
   ],
   "source": [
    "for i , sen in enumerate(tqdm(en_encoded)) :\n",
    "    \n",
    "    if len(sen) < con_size :\n",
    "        continue\n",
    "        \n",
    "    for j in range(len(sen) - con_size + 1) :\n",
    "        en_con = sen[j:j+con_size]\n",
    "        en_word2vec.append(en_con)\n",
    "        \n",
    "en_wordvec = random.shuffle(en_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "688d94bf-039e-49b8-967e-2245f44eefeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word2vec = np.array(en_word2vec)\n",
    "\n",
    "en_cen = en_word2vec[:,int(con_size/2)]\n",
    "en_neigh = np.hstack([en_word2vec[:,:int(con_size/2)],en_word2vec[:,int(con_size/2)+1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55e758b4-f73a-403b-a8fc-9fa4523b4ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape \n",
      "\n",
      "Center Shape : (704707,)\n",
      "Neighbor Shape : (704707, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Data Shape \\n')\n",
    "print('Center Shape : {}'.format(en_cen.shape))\n",
    "print('Neighbor Shape : {}'.format(en_neigh.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e4ebe-2081-43eb-bc06-9edf8a89b167",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f298731f-c41c-4b1a-b006-73f527323060",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "random.seed(20210906)\n",
    "torch.cuda.manual_seed_all(20210906)\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e86c2-dbca-4d0d-ac21-4dde0a5d7355",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82f0b06f-1494-4e5c-9e19-f5554b2fb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset) :\n",
    "\n",
    "    def __init__(self , center , neighbor , val_ratio = 0.9) :\n",
    "\n",
    "        super(Word2VecDataset , self).__init__()\n",
    "        \n",
    "        self.center = center\n",
    "        self.neighbor = neighbor\n",
    "        self.val_ratio = val_ratio\n",
    "\n",
    "    def __len__(self) :\n",
    "\n",
    "        return len(self.center)\n",
    "\n",
    "    def __getitem__(self , idx) :\n",
    "\n",
    "        cen_idx = self.center[idx]\n",
    "        neigh_idx = self.neighbor[idx]\n",
    "        \n",
    "        return cen_idx , neigh_idx\n",
    "    \n",
    "    def split_dataset(self) :\n",
    "\n",
    "        n_val = int(len(self) * self.val_ratio)\n",
    "        n_train = len(self) - n_val\n",
    "        train_set, val_set = random_split(self, [n_train, n_val])\n",
    "        return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff34984c-73ff-4d49-9061-0aa9ba6fab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Word2VecDataset(en_cen , en_neigh)\n",
    "\n",
    "train_data , val_data = dataset.split_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbfb31a0-be42-489c-9878-32fc51383b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_loader = DataLoader(train_data ,\n",
    "                          batch_size = batch_size,\n",
    "                          num_workers = 4,\n",
    "                          shuffle = True,\n",
    "                          drop_last = True)\n",
    "\n",
    "test_loader = DataLoader(val_data ,\n",
    "                         batch_size = batch_size,\n",
    "                         num_workers = 4,\n",
    "                         shuffle = False,\n",
    "                         drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f38b04-4070-4a22-9ac8-5d8079393d36",
   "metadata": {},
   "source": [
    "## Model Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29500041-f4b4-4d42-b3cc-629fbcbb2e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_size = 512\n",
    "\n",
    "v_size = en_encoder.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9edab222-6976-4ac9-9293-443ae859a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module) :\n",
    "    \n",
    "    def __init__(self , v_size , h_size , con_size) :\n",
    "        \n",
    "        super(SkipGram , self).__init__()\n",
    "        self.v_size = v_size\n",
    "        self.h_size = h_size\n",
    "        self.con_size = con_size\n",
    "        \n",
    "        self.em = nn.Embedding(num_embeddings=v_size, \n",
    "                               embedding_dim=h_size,\n",
    "                               padding_idx=0)\n",
    "        self.out = nn.Linear(h_size, v_size*(con_size-1))\n",
    "        \n",
    "    def init_param(self) :        \n",
    "        nn.init.random_normal_(self.em.weight , mean=0.0, std=0.1)\n",
    "\n",
    "        nn.init.xavier_normal_(self.out.weight)\n",
    "        nn.init.zeros_(self.out.bias)\n",
    "\n",
    "    def get_em(self) :\n",
    "        em_weight = self.em.weight.detach().cpu().numpy()\n",
    "        em_weight[0] = 0.0\n",
    "        \n",
    "        return em_weight\n",
    "        \n",
    "    def forward(self , in_tensor) :\n",
    "        in_tensor = in_tensor.unsqueeze(1)\n",
    "        em_tensor = self.em(in_tensor)\n",
    "        o_tensor = self.out(em_tensor)\n",
    "        o_tensor = torch.reshape(o_tensor, [-1,(self.con_size-1),self.v_size])\n",
    "        p_tensor = F.softmax(o_tensor, dim=-1)\n",
    "       \n",
    "        return p_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb74ebf-dee3-407b-9c4d-8d8b65b4cd0d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6846244-1ce9-4db9-999e-d14ed9880827",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_size = 20\n",
    "min_loss = 1e+7\n",
    "init_lr = 0.025\n",
    "early_count = 0\n",
    "log_count = 0\n",
    "\n",
    "skipgram = SkipGram(v_size , h_size , con_size).to(device)\n",
    "optimizer = optim.SGD(skipgram.parameters(), lr = init_lr , momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46244e30-be57-42c6-a621-896b30d1a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_fn(epoch , lr) :\n",
    "    \n",
    "    decay_lr = lr * (epoch / epoch_size)\n",
    "    \n",
    "    return (lr - decay_lr) / lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5eef815b-bc5f-4925-94ca-924fc7989215",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch : schedule_fn(epoch, init_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a4f0641-65c9-4233-b755-6bfc9dc78717",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/skipgram/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed72f32f-1452-4900-912d-eaf873af641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_fn(y_output , y_label) :\n",
    "\n",
    "    y_arg = torch.argmax(y_output, dim=-1)\n",
    "    y_acc = (y_arg == y_label).float()\n",
    "\n",
    "    y_acc = torch.mean(y_acc , dim=-1)\n",
    "    y_acc = torch.mean(y_acc)\n",
    "\n",
    "    return y_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9344f68a-0611-4d96-ae1a-aadf0e3e3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_output , y_label) :\n",
    "\n",
    "    y_label = y_label.unsqueeze(2)\n",
    "    y_prob = torch.gather(y_output, -1, y_label)\n",
    "    \n",
    "    y_loss = -torch.log(y_prob+1e-20)\n",
    "\n",
    "    y_loss = torch.mean(y_loss , dim=-1)\n",
    "    y_loss = torch.mean(y_loss)\n",
    "\n",
    "    return y_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcb1dc6d-63b9-4250-a1c7-182f675a85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressLearning(value, endvalue, loss , acc , bar_length=50):\n",
    "      \n",
    "    percent = float(value + 1) / endvalue\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}/{2} \\t Loss : {3:.3f} , Acc : {4:.3f}\".format(arrow + spaces, value+1 , endvalue , loss , acc))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad0f487b-c34b-44c3-ab61-20f1b8cefb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model , test_loader , device) :\n",
    "\n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "\n",
    "    with torch.no_grad() :\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for cen_data , neigh_data in test_loader :\n",
    "     \n",
    "            cen = cen_data.to(device)    \n",
    "            neigh = neigh_data.long().to(device)\n",
    "\n",
    "            output = model(cen)\n",
    "\n",
    "            loss_idx = loss_fn(output , neigh)\n",
    "            acc_idx = acc_fn(output , neigh)\n",
    "\n",
    "            loss += loss_idx\n",
    "            acc += acc_idx\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    loss /= len(test_loader)\n",
    "    acc /= len(test_loader)\n",
    "\n",
    "    return loss , acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb01c940-b3d2-4d97-a460-7521bd14230d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 , Learning Rate : 2.500000e-02\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 8.315 , Acc : 0.066\n",
      "Validation Loss : 8.2008 \t Validation Acc : 0.0656\n",
      "\n",
      "Epoch : 1 , Learning Rate : 2.375000e-02\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 7.868 , Acc : 0.077\n",
      "Validation Loss : 7.7996 \t Validation Acc : 0.0828\n",
      "\n",
      "Epoch : 2 , Learning Rate : 2.250000e-02\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 7.458 , Acc : 0.100\n",
      "Validation Loss : 7.5545 \t Validation Acc : 0.0910\n",
      "\n",
      "Epoch : 3 , Learning Rate : 2.125000e-02\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 7.240 , Acc : 0.098\n",
      "Validation Loss : 7.3822 \t Validation Acc : 0.0958\n",
      "\n",
      "Epoch : 4 , Learning Rate : 2.000000e-02\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 7.072 , Acc : 0.115\n",
      "Validation Loss : 7.2528 \t Validation Acc : 0.0989\n",
      "\n",
      "Epoch : 5 , Learning Rate : 1.875000e-02\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.934 , Acc : 0.101\n",
      "Validation Loss : 7.1517 \t Validation Acc : 0.1008\n",
      "\n",
      "Epoch : 6 , Learning Rate : 1.750000e-02\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.909 , Acc : 0.105\n",
      "Validation Loss : 7.0701 \t Validation Acc : 0.1022\n",
      "\n",
      "Epoch : 7 , Learning Rate : 1.625000e-02\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.777 , Acc : 0.108\n",
      "Validation Loss : 7.0034 \t Validation Acc : 0.1035\n",
      "\n",
      "Epoch : 8 , Learning Rate : 1.500000e-02\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.773 , Acc : 0.112\n",
      "Validation Loss : 6.9482 \t Validation Acc : 0.1043\n",
      "\n",
      "Epoch : 9 , Learning Rate : 1.375000e-02\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.760 , Acc : 0.112\n",
      "Validation Loss : 6.9018 \t Validation Acc : 0.1051\n",
      "\n",
      "Epoch : 10 , Learning Rate : 1.250000e-02\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.494 , Acc : 0.116\n",
      "Validation Loss : 6.8632 \t Validation Acc : 0.1056\n",
      "\n",
      "Epoch : 11 , Learning Rate : 1.125000e-02\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.567 , Acc : 0.109\n",
      "Validation Loss : 6.8308 \t Validation Acc : 0.1060\n",
      "\n",
      "Epoch : 12 , Learning Rate : 1.000000e-02\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.555 , Acc : 0.114\n",
      "Validation Loss : 6.8036 \t Validation Acc : 0.1066\n",
      "\n",
      "Epoch : 13 , Learning Rate : 8.750000e-03\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.416 , Acc : 0.113\n",
      "Validation Loss : 6.7811 \t Validation Acc : 0.1069\n",
      "\n",
      "Epoch : 14 , Learning Rate : 7.500000e-03\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.416 , Acc : 0.113\n",
      "Validation Loss : 6.7625 \t Validation Acc : 0.1072\n",
      "\n",
      "Epoch : 15 , Learning Rate : 6.250000e-03\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.466 , Acc : 0.114\n",
      "Validation Loss : 6.7476 \t Validation Acc : 0.1073\n",
      "\n",
      "Epoch : 16 , Learning Rate : 5.000000e-03\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.395 , Acc : 0.130\n",
      "Validation Loss : 6.7359 \t Validation Acc : 0.1076\n",
      "\n",
      "Epoch : 17 , Learning Rate : 3.750000e-03\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.334 , Acc : 0.121\n",
      "Validation Loss : 6.7275 \t Validation Acc : 0.1076\n",
      "\n",
      "Epoch : 18 , Learning Rate : 2.500000e-03\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.308 , Acc : 0.130\n",
      "Validation Loss : 6.7219 \t Validation Acc : 0.1077\n",
      "\n",
      "Epoch : 19 , Learning Rate : 1.250000e-03\n",
      "Percent: [------------------------------------------------->] 275/275 \t Loss : 6.305 , Acc : 0.119\n",
      "Validation Loss : 6.7191 \t Validation Acc : 0.1077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_size) :\n",
    "\n",
    "    idx = 0\n",
    "    print('Epoch : %d , Learning Rate : %e' %(epoch , optimizer.param_groups[0]['lr']))\n",
    "    \n",
    "    for cen_data , neigh_data in train_loader : \n",
    "\n",
    "        cen = cen_data.to(device)    \n",
    "        neigh = neigh_data.long().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = skipgram(cen) \n",
    "        \n",
    "        loss = loss_fn(output , neigh)\n",
    "        acc = acc_fn(output , neigh)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progressLearning(idx , len(train_loader) , loss.item() , acc.item())\n",
    "\n",
    "        if (idx + 1) % 10 == 0 :\n",
    "            \n",
    "            writer.add_scalar('train/loss' , loss.item() , log_count)\n",
    "            writer.add_scalar('train/acc' , acc.item() , log_count)\n",
    "            log_count += 1\n",
    "\n",
    "        idx += 1 \n",
    "\n",
    "    test_loss, test_acc = evaluate(skipgram, test_loader, device) \n",
    "    \n",
    "    writer.add_scalar('test/loss' , test_loss.item() , epoch)\n",
    "    writer.add_scalar('test/acc' , test_acc.item() , epoch)\n",
    "    \n",
    "    if test_loss < min_loss :\n",
    "        min_loss = test_loss\n",
    "        torch.save({'epoch' : (epoch) ,  \n",
    "                    'model_state_dict' : skipgram.state_dict() , \n",
    "                    'loss' : test_loss.item() , \n",
    "                    'acc' : test_acc.item()} , \n",
    "                    f'./Model/checkpoint_skipgram.pt')        \n",
    "        early_count = 0 \n",
    "        \n",
    "    else :\n",
    "        early_count += 1\n",
    "        if early_count >= 5 :      \n",
    "            print('\\nTraining Stopped')\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    print('\\nValidation Loss : %.4f \\t Validation Acc : %.4f\\n' %(test_loss , test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59070ae-ecd5-40d3-a198-f2c16200574e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
