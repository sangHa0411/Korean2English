{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3d8a4a-9511-4729-86a8-685dd81594d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader , Subset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc08618-d053-4353-a3b1-cfa33c352726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "921564bf-6b90-47d1-a53e-1e95d8a727df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from preprocessor.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from preprocessor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17e179df-ca3d-4cc8-a7c7-ea50fff96dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d9505f6-7161-462f-af87-f3d49bf6931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56664b34-2f11-4b8a-a110-ab4035443932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b02ee3-d796-436f-b597-b099f11c7e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a2d84-b00e-43b5-9e15-e80c215412ee",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7302e5d3-12d0-49e8-8613-0c5b2230fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '../../Data/'\n",
    "data = pd.read_excel(dir_path + '한국어_대화체_번역.xlsx' , engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4da2f22-ec0d-428f-af99-7b5989e7befc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>원문</th>\n",
       "      <th>번역문</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이번 신제품 출시에 대한 시장의 반응은 어떤가요?</td>\n",
       "      <td>How is the market's reaction to the newly rele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>판매량이 지난번 제품보다 빠르게 늘고 있습니다.</td>\n",
       "      <td>The sales increase is faster than the previous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그렇다면 공장에 연락해서 주문량을 더 늘려야겠네요.</td>\n",
       "      <td>Then, we'll have to call the manufacturer and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>네, 제가 연락해서 주문량을 2배로 늘리겠습니다.</td>\n",
       "      <td>Sure, I'll make a call and double the volume o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>지난 회의 마지막에 논의했던 안건을 다시 볼까요?</td>\n",
       "      <td>Shall we take a look at the issues we discusse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             원문  \\\n",
       "0   이번 신제품 출시에 대한 시장의 반응은 어떤가요?   \n",
       "1    판매량이 지난번 제품보다 빠르게 늘고 있습니다.   \n",
       "2  그렇다면 공장에 연락해서 주문량을 더 늘려야겠네요.   \n",
       "3   네, 제가 연락해서 주문량을 2배로 늘리겠습니다.   \n",
       "4   지난 회의 마지막에 논의했던 안건을 다시 볼까요?   \n",
       "\n",
       "                                                 번역문  \n",
       "0  How is the market's reaction to the newly rele...  \n",
       "1  The sales increase is faster than the previous...  \n",
       "2  Then, we'll have to call the manufacturer and ...  \n",
       "3  Sure, I'll make a call and double the volume o...  \n",
       "4  Shall we take a look at the issues we discusse...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "\n",
    "data[['원문','번역문']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21eb953-8b85-4734-a914-0f25b093e4ad",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07550a9f-3fcc-4248-a690-b1274e82bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = data['번역문']\n",
    "kor_data = data['원문']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87f8cb68-bc5a-46c9-bc95-24b570e6c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_encoder = Preprocessor(en_data, word_tokenize, th=3)\n",
    "en_df = pd.read_csv('./Embedding/csv/en_idx2word.csv')\n",
    "en_encoder.set_dict(en_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35ea2434-481e-46b5-9f93-bb6785b7e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_encoder = Preprocessor(kor_data, mecab.morphs, th=5)\n",
    "kor_df = pd.read_csv('./Embedding/csv/kor_idx2word.csv')\n",
    "kor_encoder.set_dict(kor_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd5ad4b7-d2dd-4d5c-b942-dbee312eda94",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_idx_data = en_encoder.encode()\n",
    "kor_idx_data = kor_encoder.encode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820e692-5ce8-42b7-b52a-3efa2e397a39",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59ab701a-4ed9-4952-993a-b7d162759985",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_len = [len(idx_data) for idx_data in en_idx_data]\n",
    "kor_len = [len(idx_data) for idx_data in kor_idx_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42ed0690-40d1-4521-8306-a948aaab0a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "axes[0].set_title('English Data Length')\n",
    "axes[0].hist(en_len, color='r', bins=100)\n",
    "\n",
    "axes[1].set_title('Korean Data Length')\n",
    "axes[1].hist(kor_len, color='b', bins=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde52af-7865-4375-9db4-2baf051562dd",
   "metadata": {},
   "source": [
    "## Data Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e51e424-05a9-4262-99de-98261cf4453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = 10\n",
    "max_size = 25\n",
    "idx_list = [i for i in range(data_size) \\\n",
    "            if (min(en_len[i],kor_len[i])>=min_size) and (max(en_len[i],kor_len[i])<=max_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f04c171-57b9-4e5d-8cfd-bc3fa26bce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_idx_data = [en_idx_data[i] for i in idx_list]\n",
    "kor_idx_data = [kor_idx_data[i] for i in idx_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e94be-38c0-401e-8afd-c66b5241ea7b",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c7448b2-6fbf-41e6-801d-06255d3b6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset) :\n",
    "\n",
    "    def __init__(self, en_index, de_index, val_ratio=0.1) :\n",
    "        super(TranslationDataset , self).__init__()\n",
    "        self.idx_data = self.build_data(en_index, de_index)\n",
    "        self.val_ratio = val_ratio\n",
    "        \n",
    "    def build_data(self, en_data, de_data) :\n",
    "        data_len = len(en_data)\n",
    "        idx_data = [(en_data[i],de_data[i]) for i in range(data_len)]\n",
    "        \n",
    "        return idx_data\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.idx_data)\n",
    "\n",
    "    def __getitem__(self , idx) :\n",
    "        return self.idx_data[idx]\n",
    "    \n",
    "    def split(self) :\n",
    "        n_val = int(len(self) * self.val_ratio)\n",
    "        n_train = len(self) - n_val\n",
    "        train_set, val_set = random_split(self, [n_train, n_val])\n",
    "        \n",
    "        return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1cf1a8d6-7a91-4214-aecf-c38eec641561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, sen_size):\n",
    "        self.sen_size = sen_size\n",
    "        \n",
    "    def __call__(self, batch_samples):\n",
    "        \n",
    "        src_tensor = []\n",
    "        tar_tensor = []\n",
    "        for src_idx, tar_idx in batch_samples:\n",
    "            src_tensor.append(torch.tensor(src_idx))\n",
    "            tar_tensor.append(torch.tensor(tar_idx))\n",
    "        \n",
    "        src_tensor = pad_sequence(src_tensor, batch_first=True, padding_value=Token.PAD_IDX.value)\n",
    "        src_tensor = F.pad(src_tensor,\n",
    "                           (0,self.sen_size-src_tensor.shape[1]),\n",
    "                           mode='constant', \n",
    "                           value=Token.PAD_IDX.value)\n",
    "    \n",
    "        tar_tensor = pad_sequence(tar_tensor, batch_first=True, padding_value=Token.PAD_IDX.value)\n",
    "        tar_tensor = F.pad(tar_tensor,\n",
    "                           (0,self.sen_size+1-tar_tensor.shape[1]),\n",
    "                           mode='constant', \n",
    "                           value=Token.PAD_IDX.value)\n",
    "    \n",
    "        return {'encoder_in' : src_tensor, \n",
    "                'decoder_in' : tar_tensor[:,:-1], \n",
    "                'decoder_out' : tar_tensor[:,1:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54c4b7a9-9f68-450b-9927-5df5760a64b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 25\n",
    "batch_size = 64\n",
    "\n",
    "dataset = TranslationDataset(kor_idx_data, en_idx_data)\n",
    "train_data, val_data = dataset.split()\n",
    "collator = Collator(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8fde3343-4836-4570-b13f-d409b855f6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data,\n",
    "                          num_workers=4,\n",
    "                          shuffle=True,\n",
    "                          batch_size=batch_size,\n",
    "                          collate_fn=collator)\n",
    "\n",
    "val_loader = DataLoader(val_data,\n",
    "                        num_workers=4,\n",
    "                        shuffle=False,\n",
    "                        batch_size=batch_size,\n",
    "                        collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4e0c2-4c37-4022-a513-432666621d0d",
   "metadata": {},
   "source": [
    "## Device & Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "122ff02d-45fe-42f2-8880-c4d08b0629c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "random.seed(20210906)\n",
    "torch.cuda.manual_seed_all(20210906)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed26eb4-4524-4c40-9421-62d25278837d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8385ddf1-9fc9-483a-ab20-fca7d280ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingMask(nn.Module) :\n",
    "\n",
    "    def __init__(self, sen_size) :\n",
    "        super(PaddingMask , self).__init__() \n",
    "        self.sen_size = sen_size\n",
    "    \n",
    "    def forward(self, in_tensor) :\n",
    "        batch_size = in_tensor.shape[0]\n",
    "        # mask tensor which element is 0.0\n",
    "        flag_tensor = torch.where(in_tensor == 0.0 , 1.0 , 0.0)\n",
    "        # shape : (batch_size, 1, 1, sen_size)\n",
    "        flag_tensor = torch.reshape(flag_tensor , (batch_size, 1, 1, self.sen_size)) \n",
    "        \n",
    "        return flag_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00ca3d19-98ab-46c6-aaa3-2404596ac905",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookAheadMask(nn.Module) :\n",
    "\n",
    "    def __init__(self, sen_size, cuda_flag) :\n",
    "        super(LookAheadMask, self).__init__() \n",
    "        self.sen_size = sen_size\n",
    "        self.mask_tensor = self.get_mask(sen_size).cuda() if cuda_flag else self.get_mask(sen_size)\n",
    "\n",
    "    def get_mask(self, sen_size) :\n",
    "        # masking tensor\n",
    "        mask_array = 1 - np.tril(np.ones((sen_size,sen_size)) , 0)\n",
    "        mask_tensor = torch.tensor(mask_array , dtype = torch.float32 , requires_grad=False)\n",
    "        mask_tensor = mask_tensor.unsqueeze(0) # shape : (1, sen_size, sen_size)\n",
    "\n",
    "        return mask_tensor\n",
    "    \n",
    "    def forward(self, in_tensor) :\n",
    "        lookahead_mask_tensor = torch.maximum(in_tensor, self.mask_tensor)\n",
    "\n",
    "        return lookahead_mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7de3f039-c5e6-4bd7-a635-f27a6017d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module) :\n",
    "\n",
    "    def __init__(self, pos_len, d_model, cuda_flag) :\n",
    "        super(PositionalEncoding , self).__init__()\n",
    "        self.pos_len = pos_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # w : weight\n",
    "        # pe : Encoding tensor\n",
    "        if cuda_flag == True :\n",
    "            self.w = torch.sqrt(torch.tensor(d_model, dtype=torch.float32, requires_grad=False)).cuda()\n",
    "            self.pe = self.get_embedding(pos_len, d_model).cuda()\n",
    "\n",
    "        else :\n",
    "            self.w = torch.sqrt(torch.tensor(d_model, dtype=torch.float32, requires_grad=False))\n",
    "            self.pe = self.get_embedding(pos_len, d_model)\n",
    "\n",
    "    # Embedding tensor : (batch_size, sen_size, embedding_dim)\n",
    "    # Making Encoding tensor (1, sen_size, embedding_dim)\n",
    "    def get_embedding(self, pos_len, d_model) :\n",
    "        pos_vec = torch.arange(pos_len).float()\n",
    "        pos_vec = pos_vec.unsqueeze(1)\n",
    "\n",
    "        i_vec = torch.arange(d_model).float() / 2\n",
    "        i_vec = torch.floor(i_vec) * 2\n",
    "        i_vec = i_vec.unsqueeze(0) / d_model\n",
    "        i_vec = 1 / torch.pow(1e+4 , i_vec)\n",
    "\n",
    "        em = torch.mul(pos_vec, i_vec)\n",
    "        pe = torch.zeros(pos_len, d_model, requires_grad=False)\n",
    "        sin_em = torch.sin(em)\n",
    "        cos_em = torch.cos(em)\n",
    "\n",
    "        pe[:,::2] = sin_em[:,::2]\n",
    "        pe[:,1::2] = cos_em[:,1::2]\n",
    "\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, in_tensor) :\n",
    "        en_tensor = (in_tensor * self.w) + self.pe\n",
    "        \n",
    "        return en_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c5675306-fa27-4b52-9f51-76529149cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module) :\n",
    "\n",
    "    def __init__(self, sen_size,  d_model, num_heads) :\n",
    "        super(MultiHeadAttention , self).__init__()\n",
    "        self.sen_size = sen_size # sen_size\n",
    "        self.d_model = d_model # embedidng_dim\n",
    "        self.num_heads = num_heads # head_size\n",
    "        self.depth = int(d_model / num_heads) # embedding_dim / num_heads\n",
    "\n",
    "        self.q_layer = nn.Linear(d_model, d_model)\n",
    "        self.k_layer = nn.Linear(d_model, d_model)\n",
    "        self.v_layer = nn.Linear(d_model, d_model)\n",
    "        self.o_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.tensor(self.depth , dtype=torch.float32 , requires_grad=False))\n",
    "        \n",
    "        self.init_param()\n",
    "\n",
    "    def split(self, tensor) :\n",
    "        tensor = torch.reshape(tensor, (-1, self.sen_size, self.num_heads, self.depth)) # (batch_size, sen_size, num_heads, depth)\n",
    "        tensor = tensor.permute(0,2,1,3) # (batch_size, num_heads, sen_size, depth)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def merge(self, tensor) :\n",
    "        tensor = tensor.permute(0,2,1,3) # (batch_size, sen_size, num_heads, depth)\n",
    "        tensor = torch.reshape(tensor, (-1, self.sen_size, self.d_model)) # (batch_size , sen_size , embedding_dim)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def scaled_dot_production(self, q_tensor, k_tensor, v_tensor, m_tensor) :\n",
    "        q_tensor = self.split(q_tensor)\n",
    "        k_tensor = self.split(k_tensor)\n",
    "        v_tensor = self.split(v_tensor)\n",
    "        \n",
    "        k_tensor_T = k_tensor.permute(0,1,3,2) # (batch_size, num_heads, depth, sen_size)\n",
    "\n",
    "        qk_tensor = torch.matmul(q_tensor , k_tensor_T) # (batch_size, num_heads, sen_size, sen_size)\n",
    "        qk_tensor /= self.scale\n",
    "\n",
    "        # pad mask tensor shape : (batch_size, 1, 1, sen_size)\n",
    "        # lookahead mask tensor shape : (batch_size, 1, sen_size, sen_size)\n",
    "        if m_tensor != None :\n",
    "            qk_tensor -= (m_tensor * 1e+6)\n",
    "\n",
    "        qk_tensor = F.softmax(qk_tensor, dim=-1)\n",
    "        att = torch.matmul(qk_tensor, v_tensor) # (batch_size, num_heads, sen_size, depth)\n",
    "\n",
    "        return att\n",
    "\n",
    "    # Xavier Initialization\n",
    "    def init_param(self) :\n",
    "        for m in self.modules() :\n",
    "            if isinstance(m,nn.Linear) :\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, q_in, k_in, v_in, m_in) :\n",
    "        q_tensor = self.q_layer(q_in)\n",
    "        k_tensor = self.k_layer(k_in)\n",
    "        v_tensor = self.v_layer(v_in)\n",
    "\n",
    "        att_tensor = self.scaled_dot_production(q_tensor , k_tensor , v_tensor , m_in)\n",
    "        concat_tensor = self.merge(att_tensor)\n",
    "\n",
    "        o_tensor = self.o_layer(concat_tensor)\n",
    "\n",
    "        return o_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dea414d7-8b60-4d84-b32d-778f8ab400e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module) :\n",
    "\n",
    "    def __init__(self, hidden_size, d_model) :\n",
    "        super(FeedForward , self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # relu activation and input, output dim are same\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model , hidden_size), \n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(hidden_size , d_model))\n",
    "        self.init_param()\n",
    "                \n",
    "    # He Initialization\n",
    "    def init_param(self) :\n",
    "        gain = 2 ** (1/2)\n",
    "        for m in self.modules() :\n",
    "            if isinstance(m , nn.Linear) :\n",
    "                nn.init.kaiming_normal_(m.weight , gain)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self , in_tensor) :\n",
    "        o_tensor = self.ff(in_tensor)\n",
    "\n",
    "        return o_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd374424-a093-4146-b5ba-60201260dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module) :\n",
    "\n",
    "    def __init__(self, layer_size, sen_size, v_size, d_model, num_heads, hidden_size, \n",
    "                 drop_rate, norm_rate, cuda_flag) :\n",
    "        super(TransformerEncoder , self).__init__()\n",
    "        self.layer_size = layer_size\n",
    "        self.sen_size = sen_size\n",
    "        self.v_size = v_size\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.drop_rate = drop_rate\n",
    "        self.norm_rate = norm_rate\n",
    "\n",
    "        self.em = nn.Embedding(num_embeddings=v_size, embedding_dim=d_model, padding_idx=0) # embedding\n",
    "        self.pos = PositionalEncoding(sen_size, d_model, cuda_flag) # positional encoding\n",
    "        self.pad = PaddingMask(sen_size) # masking\n",
    "        \n",
    "        self.mha_layer = nn.ModuleList()\n",
    "        self.ff_layer = nn.ModuleList()\n",
    "\n",
    "        self.drop_layer = nn.Dropout(drop_rate)\n",
    "        self.norm_layer = nn.LayerNorm(d_model , eps=norm_rate)\n",
    "\n",
    "        for i in range(layer_size) :\n",
    "            self.mha_layer.append(MultiHeadAttention(sen_size , d_model , num_heads))\n",
    "            self.ff_layer.append(FeedForward(hidden_size , d_model))\n",
    "\n",
    "        self.init_param()\n",
    "        \n",
    "    def set_embedding(self, em_weight) :\n",
    "        em_v_size, em_h_size = em_weight.shape\n",
    "        assert em_v_size == self.v_size\n",
    "        assert em_h_size == self.d_model\n",
    "        em_weight = torch.tensor(em_weight)\n",
    "        self.em = nn.Embedding.from_pretrained(em_weight,\n",
    "                                               freeze=True,\n",
    "                                               padding_idx=0)\n",
    "            \n",
    "    def init_param(self) :\n",
    "        nn.init.normal_(self.em.weight, mean=0.0, std=0.1)\n",
    "            \n",
    "    def forward_block(self, i, in_tensor, m_tensor) :\n",
    "        mha_tensor = self.mha_layer[i](in_tensor , in_tensor , in_tensor , m_tensor)\n",
    "        mha_tensor = self.drop_layer(mha_tensor)\n",
    "        h_tensor = self.norm_layer(in_tensor + mha_tensor)\n",
    "\n",
    "        ff_tensor = self.ff_layer[i](h_tensor)\n",
    "        ff_tensor = self.drop_layer(ff_tensor)\n",
    "        o_tensor = self.norm_layer(h_tensor + ff_tensor)\n",
    "\n",
    "        return o_tensor\n",
    "\n",
    "    def forward(self, in_tensor) :\n",
    "        pad_mask = self.pad(in_tensor)\n",
    "        em_tensor = self.em(in_tensor)\n",
    "        en_tensor = self.pos(em_tensor)\n",
    "        en_tensor = self.drop_layer(en_tensor)\n",
    "        \n",
    "        tensor_ptr = en_tensor\n",
    "        for i in range(self.layer_size) :\n",
    "            tensor_ptr = self.forward_block(i, tensor_ptr, pad_mask)\n",
    "        \n",
    "        return tensor_ptr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a1db92ae-70b8-4580-83b6-ece97c4257a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module) :\n",
    "\n",
    "    def __init__(self, layer_size, sen_size, v_size, d_model, num_heads, hidden_size, \n",
    "                 drop_rate, norm_rate, cuda_flag) :\n",
    "        super(TransformerDecoder , self).__init__()\n",
    "        self.layer_size = layer_size\n",
    "        self.sen_size = sen_size\n",
    "        self.v_size = v_size\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.drop_rate = drop_rate\n",
    "        self.norm_rate = norm_rate\n",
    "        \n",
    "        self.em = nn.Embedding(num_embeddings=v_size, embedding_dim=d_model, padding_idx=0) # embedding\n",
    "        self.pos = PositionalEncoding(sen_size, d_model, cuda_flag) # positional encoding\n",
    "        self.pad = PaddingMask(sen_size) # padding masking\n",
    "        self.lookahead = LookAheadMask(sen_size , cuda_flag) # lookahead masking\n",
    "        \n",
    "        self.masked_mha_layer = nn.ModuleList()\n",
    "        self.mha_layer = nn.ModuleList()\n",
    "        self.ff_layer = nn.ModuleList()\n",
    "\n",
    "        self.drop_layer = nn.Dropout(drop_rate)\n",
    "        self.norm_layer = nn.LayerNorm(d_model , eps=norm_rate)\n",
    "        \n",
    "        self.o_layer = nn.Linear(d_model, v_size)\n",
    "\n",
    "        for i in range(layer_size) :\n",
    "            self.masked_mha_layer.append(MultiHeadAttention(sen_size , d_model , num_heads))\n",
    "            self.mha_layer.append(MultiHeadAttention(sen_size , d_model , num_heads))\n",
    "            self.ff_layer.append(FeedForward(hidden_size , d_model))\n",
    "    \n",
    "        self.init_param()\n",
    "        \n",
    "    def init_param(self) :\n",
    "        nn.init.normal_(self.em.weight, mean=0.0, std=0.1)\n",
    "        nn.init.xavier_normal_(self.o_layer.weight, gain=1.0)\n",
    "        \n",
    "    def set_embedding(self, em_weight, bias) :\n",
    "        em_v_size, em_h_size = em_weight.shape\n",
    "        assert em_v_size == self.v_size\n",
    "        assert em_h_size == self.d_model\n",
    "        em_weight = torch.tensor(em_weight)\n",
    "        bias = torch.tensor(bias)\n",
    "\n",
    "        # embedding layer from pretrained : not trainable\n",
    "        self.em = nn.Embedding.from_pretrained(em_weight,\n",
    "                                               freeze=True,\n",
    "                                               padding_idx=0)\n",
    "        \n",
    "        # output layer from pretrained : not trainable\n",
    "        self.o_layer = nn.Linear(self.d_model, self.v_size)\n",
    "        self.o_layer.weight = nn.Parameter(em_weight, requires_grad=False)\n",
    "        self.o_layer.bias = nn.Parameter(bias, requires_grad=False)\n",
    "            \n",
    "    def forward_block(self, i, in_tensor, en_out_tensor, padding_mask, lookahead_mask) :\n",
    "        # query : in_tensor\n",
    "        # key : in_tensor \n",
    "        # value : in_tensor \n",
    "        # mask ; look ahead mask\n",
    "        m_mha_tensor = self.masked_mha_layer[i](in_tensor , in_tensor , in_tensor , lookahead_mask)\n",
    "        m_mha_tensor = self.drop_layer(m_mha_tensor)\n",
    "        h_tensor = self.norm_layer(in_tensor + m_mha_tensor)\n",
    "\n",
    "        # query : output of masked multihead attention\n",
    "        # key : encoder output , \n",
    "        # value : encoder output , \n",
    "        # mask ; padding mask\n",
    "        mha_tensor = self.mha_layer[i](h_tensor, en_out_tensor, en_out_tensor, padding_mask)\n",
    "        mha_tensor = self.drop_layer(mha_tensor)\n",
    "        a_tensor = self.norm_layer(h_tensor+mha_tensor)\n",
    "\n",
    "        ff_tensor = self.ff_layer[i](a_tensor)\n",
    "        ff_tensor = self.drop_layer(ff_tensor)\n",
    "        o_tensor = self.norm_layer(a_tensor+ff_tensor)\n",
    "\n",
    "        return o_tensor\n",
    "\n",
    "    def forward(self, in_tensor, en_out_tensor) :\n",
    "        pad_mask = self.pad(in_tensor)\n",
    "        lookahead_mask = self.lookahead(pad_mask)\n",
    "        em_tensor = self.em(in_tensor)\n",
    "        de_tensor = self.pos(em_tensor)\n",
    "        de_tensor = self.drop_layer(de_tensor)\n",
    "        \n",
    "        tensor_ptr = de_tensor\n",
    "        for i in range(self.layer_size) :\n",
    "            tensor_ptr = self.forward_block(i, tensor_ptr, en_out_tensor, pad_mask, lookahead_mask)\n",
    "        o_tensor = self.o_layer(tensor_ptr)\n",
    "        \n",
    "        return o_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41111527-b748-4ea7-8d28-38fc98ea6de1",
   "metadata": {},
   "source": [
    "## Model Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "34e2ebd6-6db0-44b2-8197-88c8a768da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_v_size = len(en_df)\n",
    "kor_v_size = len(kor_df)\n",
    "\n",
    "# transformer model\n",
    "layer_size = 6\n",
    "sen_size = max_len\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "h_size = 2048\n",
    "drop_rate = 1e-1\n",
    "norm_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4a32cc66-d3cd-4dda-829d-03ceb495d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(layer_size, sen_size, kor_v_size, \n",
    "                             d_model, num_heads, h_size, drop_rate, norm_rate, use_cuda)\n",
    "kor_weight = np.load('./Embedding/array/kor_weight.npy')\n",
    "encoder.set_embedding(kor_weight)\n",
    "\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e78c5fa-e4ea-470b-8359-3d4086a3df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerDecoder(layer_size, sen_size, en_v_size, \n",
    "                             d_model, num_heads, h_size, drop_rate, norm_rate, use_cuda)\n",
    "en_weight = np.load('./Embedding/array/en_weight.npy')\n",
    "en_bias = np.load('./Embedding/array/en_bias.npy')\n",
    "decoder.set_embedding(en_weight,en_bias)\n",
    "\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beeb5a7-fcdf-4cf4-9685-351d1ccb1605",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "605b9618-1a19-4fb6-aca1-eebc7ff3392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_lr = 1e-4 # no meaning\n",
    "\n",
    "epoch_size = int(100000 / len(train_loader))\n",
    "warmup_steps = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff579e67-7775-4655-8ded-a26de77216a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_fn(epoch, d_model, dumb_lr) :\n",
    "    step_num = epoch + 1\n",
    "    val1 = d_model ** (-0.5)\n",
    "    \n",
    "    arg1 = step_num ** (-0.5)\n",
    "    arg2 = (warmup_steps ** (-1.5)) * step_num\n",
    "    \n",
    "    val2 = min(arg1 , arg2) \n",
    "    return (val1 * val2) / dumb_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "57880e66-2a15-473a-b3f0-dac6a4f3c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_param = chain(encoder.parameters(), decoder.parameters())\n",
    "\n",
    "optimizer = optim.Adam(tf_param, lr=dumb_lr, betas=(0.9,0.98), eps=1e-9)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, \n",
    "                                        lr_lambda = lambda epoch: schedule_fn(epoch, d_model, dumb_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd58022f-68c0-48cb-a5aa-0abeee8c04e0",
   "metadata": {},
   "source": [
    "## Acc & Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7674d10b-b778-4e8d-8968-c7e97c397210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_fn(y_output , y_label) :\n",
    "    y_arg = torch.argmax(y_output, dim=-1)\n",
    "    y_acc = (y_arg == y_label).float()\n",
    "    y_acc = torch.mean(y_acc)\n",
    "    return y_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "60f934ae-b533-4c39-b58d-76a4275ecee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fb6adb-fc5d-4eaa-a93b-eaf41c3a0796",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "55dc8431-65d7-4f84-bcc7-b0c2a5fede3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('./Log/runs/transformer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0a6849-f33f-455c-8d41-2177c5137e4d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "818615d4-75e6-46a2-bf30-a52a70ac4651",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = np.inf\n",
    "stop_count = 0\n",
    "log_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "14b1a8fe-0ccf-4867-b97b-1484de12e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressLearning(epoch, value, endvalue, bar_length=50):\n",
    "    percent = float(value + 1) / endvalue\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "\n",
    "    sys.stdout.write(\"\\rEpoch [{0}] : [{1}] {2}/{3}\".format(epoch, \n",
    "                                                            arrow + spaces, \n",
    "                                                            value+1 , \n",
    "                                                            endvalue))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6dc38a61-8290-4ce8-a49c-66f2d382e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, test_loader) :\n",
    "    with torch.no_grad() :\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        loss_eval = 0.0\n",
    "        acc_eval = 0.0\n",
    "        \n",
    "        for data in test_loader :\n",
    "            en_in = data['encoder_in'].long().to(device)\n",
    "            de_in = data['decoder_in'].long().to(device)\n",
    "            de_label = data['decoder_out'].long().to(device)\n",
    "            \n",
    "            de_label = data['decoder_out'].long().to(device)\n",
    "            de_label = torch.reshape(de_label, (-1,))\n",
    "            \n",
    "            en_output = encoder(en_in)\n",
    "            de_output = decoder(de_in, en_output)\n",
    "            de_output = torch.reshape(de_output, (-1,en_v_size))\n",
    "            \n",
    "            loss_eval += loss_fn(de_output , de_label)\n",
    "            acc_eval += acc_fn(de_output , de_label)\n",
    "\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        loss_eval /= len(test_loader)\n",
    "        acc_eval /= len(test_loader)\n",
    "        \n",
    "    return loss_eval , acc_eval  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6ca61e80-5052-4c67-8e04-aff681fc8374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] : [------------------------------------------------->] 1295/1295\tVal Loss : 7.595 \t Val Accuracy : 0.001\n",
      "Epoch [1] : [------------------------------------------------->] 1295/1295\tVal Loss : 5.638 \t Val Accuracy : 0.354\n",
      "Epoch [2] : [------------------------------------------------->] 1295/1295\tVal Loss : 4.743 \t Val Accuracy : 0.392\n",
      "Epoch [3] : [------------------------------------------------->] 1295/1295\tVal Loss : 4.168 \t Val Accuracy : 0.450\n",
      "Epoch [4] : [------------------------------------------------->] 1295/1295\tVal Loss : 3.988 \t Val Accuracy : 0.479\n",
      "Epoch [5] : [------------------------------------------------->] 1295/1295\tVal Loss : 3.880 \t Val Accuracy : 0.483\n",
      "Epoch [6] : [------------------------------------------------->] 1295/1295\tVal Loss : 3.748 \t Val Accuracy : 0.491\n",
      "Epoch [7] : [------------------------------------------------->] 1295/1295\tVal Loss : 3.615 \t Val Accuracy : 0.502\n",
      "Epoch [8] : [------------------------------------------------->] 1295/1295\tVal Loss : 3.459 \t Val Accuracy : 0.518\n",
      "Epoch [9] : [------------------------------------------------->] 1295/1295\tVal Loss : 3.335 \t Val Accuracy : 0.528\n",
      "Epoch [10] : [------------------------------------------------->] 1295/1295\tVal Loss : 3.233 \t Val Accuracy : 0.536\n",
      "Epoch [11] : [------------------------------------------------->] 1295/1295\tVal Loss : 3.141 \t Val Accuracy : 0.544\n",
      "Epoch [12] : [------------------------------------------------->] 1295/1295\tVal Loss : 3.072 \t Val Accuracy : 0.549\n",
      "Epoch [13] : [------------------------------------------------->] 1295/1295\tVal Loss : 3.002 \t Val Accuracy : 0.555\n",
      "Epoch [14] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.944 \t Val Accuracy : 0.559\n",
      "Epoch [15] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.889 \t Val Accuracy : 0.564\n",
      "Epoch [16] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.839 \t Val Accuracy : 0.569\n",
      "Epoch [17] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.789 \t Val Accuracy : 0.573\n",
      "Epoch [18] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.753 \t Val Accuracy : 0.576\n",
      "Epoch [19] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.712 \t Val Accuracy : 0.581\n",
      "Epoch [20] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.674 \t Val Accuracy : 0.584\n",
      "Epoch [21] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.654 \t Val Accuracy : 0.586\n",
      "Epoch [22] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.608 \t Val Accuracy : 0.591\n",
      "Epoch [23] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.579 \t Val Accuracy : 0.594\n",
      "Epoch [24] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.542 \t Val Accuracy : 0.599\n",
      "Epoch [25] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.512 \t Val Accuracy : 0.602\n",
      "Epoch [26] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.486 \t Val Accuracy : 0.605\n",
      "Epoch [27] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.464 \t Val Accuracy : 0.607\n",
      "Epoch [28] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.457 \t Val Accuracy : 0.606\n",
      "Epoch [29] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.414 \t Val Accuracy : 0.612\n",
      "Epoch [30] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.414 \t Val Accuracy : 0.611\n",
      "Epoch [31] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.370 \t Val Accuracy : 0.617\n",
      "Epoch [32] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.358 \t Val Accuracy : 0.618\n",
      "Epoch [33] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.321 \t Val Accuracy : 0.622\n",
      "Epoch [34] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.320 \t Val Accuracy : 0.622\n",
      "Epoch [35] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.300 \t Val Accuracy : 0.624\n",
      "Epoch [36] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.273 \t Val Accuracy : 0.627\n",
      "Epoch [37] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.254 \t Val Accuracy : 0.629\n",
      "Epoch [38] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.244 \t Val Accuracy : 0.630\n",
      "Epoch [39] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.224 \t Val Accuracy : 0.632\n",
      "Epoch [40] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.209 \t Val Accuracy : 0.634\n",
      "Epoch [41] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.189 \t Val Accuracy : 0.636\n",
      "Epoch [42] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.188 \t Val Accuracy : 0.637\n",
      "Epoch [43] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.166 \t Val Accuracy : 0.639\n",
      "Epoch [44] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.151 \t Val Accuracy : 0.640\n",
      "Epoch [45] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.146 \t Val Accuracy : 0.641\n",
      "Epoch [46] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.118 \t Val Accuracy : 0.644\n",
      "Epoch [47] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.113 \t Val Accuracy : 0.645\n",
      "Epoch [48] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.103 \t Val Accuracy : 0.646\n",
      "Epoch [49] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.092 \t Val Accuracy : 0.646\n",
      "Epoch [50] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.072 \t Val Accuracy : 0.649\n",
      "Epoch [51] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.077 \t Val Accuracy : 0.647\n",
      "Epoch [52] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.080 \t Val Accuracy : 0.647\n",
      "Epoch [53] : [------------------------------------------------->] 1295/1295\tVal Loss : 2.047 \t Val Accuracy : 0.652\n",
      "Epoch [54] : [-->                                               ] 65/1295"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-bfa2db0cc4fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0men_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mde_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mde_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_v_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-24d92daeaea2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in_tensor, en_out_tensor)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mtensor_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mde_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mtensor_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_ptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_out_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookahead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mo_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-24d92daeaea2>\u001b[0m in \u001b[0;36mforward_block\u001b[0;34m(self, i, in_tensor, en_out_tensor, padding_mask, lookahead_mask)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# value : in_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# mask ; look ahead mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mm_mha_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_mha_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tensor\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0min_tensor\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0min_tensor\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlookahead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mm_mha_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_mha_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mh_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tensor\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm_mha_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-6ff30b5d66f1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q_in, k_in, v_in, m_in)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mv_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0matt_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaled_dot_production\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_tensor\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mk_tensor\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mv_tensor\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mm_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mconcat_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-6ff30b5d66f1>\u001b[0m in \u001b[0;36mscaled_dot_production\u001b[0;34m(self, q_tensor, k_tensor, v_tensor, m_tensor)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# lookahead mask tensor shape : (batch_size, 1, sen_size, sen_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mm_tensor\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mqk_tensor\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm_tensor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e+6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mqk_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqk_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_size) :\n",
    "    idx = 0\n",
    "    for data in train_loader :\n",
    "        en_in = data['encoder_in'].long().to(device)\n",
    "        de_in = data['decoder_in'].long().to(device)\n",
    "        \n",
    "        de_label = data['decoder_out'].long().to(device)\n",
    "        de_label = torch.reshape(de_label, (-1,))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        en_output = encoder(en_in)\n",
    "        de_output = decoder(de_in, en_output)\n",
    "        de_output = torch.reshape(de_output, (-1,en_v_size))\n",
    "\n",
    "        loss = loss_fn(de_output , de_label)\n",
    "        acc = acc_fn(de_output , de_label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progressLearning(epoch, idx, len(train_loader))\n",
    "\n",
    "        if (idx + 1) % 10 == 0 :\n",
    "            writer.add_scalar('train/loss', loss.item(), log_count)\n",
    "            writer.add_scalar('train/acc', acc.item(), log_count)\n",
    "            log_count += 1\n",
    "        idx += 1\n",
    "\n",
    "    val_loss, val_acc = evaluate(encoder, decoder, val_loader)\n",
    "        \n",
    "    writer.add_scalar('test/loss', val_loss.item(), epoch)\n",
    "    writer.add_scalar('test/acc', val_acc.item(), epoch)\n",
    "    \n",
    "    if val_loss < min_loss :\n",
    "        min_loss = val_loss\n",
    "        torch.save({'epoch' : (epoch) ,  \n",
    "                    'encoder_state_dict' : encoder.state_dict() , \n",
    "                    'decoder_state_dict' : decoder.state_dict() , \n",
    "                    'loss' : val_loss.item() , \n",
    "                    'acc' : val_acc.item()} , \n",
    "                    f'./Model/checkpoint_transformer.pt')        \n",
    "        stop_count = 0 \n",
    "    else :\n",
    "        stop_count += 1\n",
    "        if stop_count >= 5 :      \n",
    "            print('\\tTraining Early Stopped')\n",
    "            break\n",
    "            \n",
    "    scheduler.step()\n",
    "    print('\\tVal Loss : %.3f \\t Val Accuracy : %.3f' %(val_loss, val_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8df83e-4df6-47bd-8e6b-883409dea631",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d6e2c-f1c1-4ed7-902d-0a0d457b708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translation :\n",
    "    \n",
    "    def __init__(self, sen_size, start_tok, en_tok, encoder, decoder , alpha=0.6) :\n",
    "        \n",
    "        self.sen_size = sen_size\n",
    "        self.en_tok = en_tok\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.encoder = encoder.eval()\n",
    "        self.decoder = decoder.eval()\n",
    "        \n",
    "    def convert(self, in_seq) :\n",
    "        in_vec = np.array(in_seq)\n",
    "        in_vec = np.pad(in_vec, [0,self.sen_size-len(in_vec)])\n",
    "        \n",
    "        in_tensor = torch.tensor(in_vec, dtype=torch.long, requires_grad=False).unsqueeze(0)\n",
    "        in_tensor = in_tensor.to(device)\n",
    "        \n",
    "        return in_tensor\n",
    "    \n",
    "    def select(self, sen_list, beam_size) :\n",
    "        sen_list.sort(key=lambda x:x[1], reverse=True)\n",
    "        sen_sel = sen_list[:beam_size]\n",
    "        \n",
    "        return sen_sel\n",
    "    \n",
    "    def check_end(self, sen_list) :\n",
    "        flag = True\n",
    "        for sen in sen_list :\n",
    "            if sen[2] == False :\n",
    "                return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def get_penalty(self, token_next) :\n",
    "        token_len = len(token_next)\n",
    "        \n",
    "        num = np.power((5+token_len), self.alpha)\n",
    "        denom = np.power((5+1), self.alpha)\n",
    "        \n",
    "        penalty = num / denom\n",
    "        \n",
    "        return penalty\n",
    "        \n",
    "    def translate(self, in_seq , beam_size=5) :\n",
    "        en_in = self.convert(in_seq)\n",
    "        en_out = self.encoder(en_in)\n",
    "        \n",
    "        sen_list = [([start_tok],0.0,False)]\n",
    "        idx = 0\n",
    "        \n",
    "        while(idx<self.sen_size) :\n",
    "            sen_ptr = []\n",
    "            \n",
    "            if self.check_end(sen_list) :\n",
    "                break\n",
    "            \n",
    "            for i in range(len(sen_list)) :\n",
    "                tok_list, val, end_flag = sen_list[i]\n",
    "                \n",
    "                if end_flag == True : \n",
    "                    sen_ptr.append(sen_list[i])\n",
    "                    continue\n",
    "                \n",
    "                de_in = self.convert(tok_list)\n",
    "                de_out = self.decoder(de_in, en_out).squeeze(0)\n",
    "                de_prob = F.softmax(de_out, dim=-1)\n",
    "                de_log = torch.log(de_prob + 1e-30)\n",
    "            \n",
    "                de_val, de_arg = torch.sort(de_log, dim=-1, descending=True)\n",
    "                \n",
    "                de_val_idx = de_val[idx]\n",
    "                de_arg_idx = de_arg[idx]\n",
    "\n",
    "                for j in range(beam_size) :        \n",
    "                    val_next = val + de_val_idx[j].item()\n",
    "                    token_next = de_arg_idx[j].item()\n",
    "                    flag_next = True if token_next == self.en_tok else False\n",
    "                    \n",
    "                    tok_list_next = tok_list + [token_next]\n",
    "                    penalty = self.get_penalty(tok_list_next)\n",
    "                    \n",
    "                    val_next /= penalty\n",
    "                    sen_ptr.append((tok_list_next, val_next, flag_next))\n",
    "                \n",
    "            sen_list = self.select(sen_ptr, beam_size)\n",
    "            \n",
    "            idx += 1\n",
    "            \n",
    "        pred_sen = sen_list[0][0]\n",
    "            \n",
    "        return pred_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02acfba4-265a-4f68-af0a-c7f1d6aa8164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453b0fb-e017-44df-8f73-c2bb66ccdc47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dbdfd2-22a1-4673-b302-08f9c9d22e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
