{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3d8a4a-9511-4729-86a8-685dd81594d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader , Subset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc08618-d053-4353-a3b1-cfa33c352726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e77c592a-a772-4a15-ab5a-7dd47cb86377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58045956-17ec-4294-bf72-204ffca23c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "921564bf-6b90-47d1-a53e-1e95d8a727df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6403fa6-bdaf-4f91-b9e6-33201dc8b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56664b34-2f11-4b8a-a110-ab4035443932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from konlpy.tag import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b02ee3-d796-436f-b597-b099f11c7e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a2d84-b00e-43b5-9e15-e80c215412ee",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7302e5d3-12d0-49e8-8613-0c5b2230fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '../../Data/'\n",
    "data = pd.read_excel(dir_path + '한국어_대화체_번역.xlsx' , engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4da2f22-ec0d-428f-af99-7b5989e7befc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>원문</th>\n",
       "      <th>번역문</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이번 신제품 출시에 대한 시장의 반응은 어떤가요?</td>\n",
       "      <td>How is the market's reaction to the newly rele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>판매량이 지난번 제품보다 빠르게 늘고 있습니다.</td>\n",
       "      <td>The sales increase is faster than the previous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그렇다면 공장에 연락해서 주문량을 더 늘려야겠네요.</td>\n",
       "      <td>Then, we'll have to call the manufacturer and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>네, 제가 연락해서 주문량을 2배로 늘리겠습니다.</td>\n",
       "      <td>Sure, I'll make a call and double the volume o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>지난 회의 마지막에 논의했던 안건을 다시 볼까요?</td>\n",
       "      <td>Shall we take a look at the issues we discusse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             원문  \\\n",
       "0   이번 신제품 출시에 대한 시장의 반응은 어떤가요?   \n",
       "1    판매량이 지난번 제품보다 빠르게 늘고 있습니다.   \n",
       "2  그렇다면 공장에 연락해서 주문량을 더 늘려야겠네요.   \n",
       "3   네, 제가 연락해서 주문량을 2배로 늘리겠습니다.   \n",
       "4   지난 회의 마지막에 논의했던 안건을 다시 볼까요?   \n",
       "\n",
       "                                                 번역문  \n",
       "0  How is the market's reaction to the newly rele...  \n",
       "1  The sales increase is faster than the previous...  \n",
       "2  Then, we'll have to call the manufacturer and ...  \n",
       "3  Sure, I'll make a call and double the volume o...  \n",
       "4  Shall we take a look at the issues we discusse...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['원문','번역문']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21eb953-8b85-4734-a914-0f25b093e4ad",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07550a9f-3fcc-4248-a690-b1274e82bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = data['번역문']\n",
    "kor_data = data['원문']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cc61f2f-eb6f-4262-8291-1c85987339c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:13<00:00, 7288.73it/s]\n"
     ]
    }
   ],
   "source": [
    "en_vs = preprocessor.VocabSet(word_tokenize, th=3)\n",
    "en_tokens = en_vs.tokens(en_data)\n",
    "en_encoder = preprocessor.Encoder(en_data, word_tokenize, en_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d0537a-d263-4af0-bf41-5ad3b3c52951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:14<00:00, 7116.53it/s]\n"
     ]
    }
   ],
   "source": [
    "en_encoded = en_encoder.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c3dbb08-162f-4586-8e23-9874eae1c29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:08<00:00, 12279.33it/s]\n"
     ]
    }
   ],
   "source": [
    "kor_vs = preprocessor.VocabSet(mecab.morphs, th=3)\n",
    "kor_tokens = kor_vs.tokens(kor_data)\n",
    "kor_encoder = preprocessor.Encoder(kor_data, mecab.morphs, kor_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ec7a5ad-cc8a-4404-80e5-cb91441eb4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:08<00:00, 11658.67it/s]\n"
     ]
    }
   ],
   "source": [
    "kor_encoded = kor_encoder.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e8394cc-ea39-4dd2-926c-855488f7633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_len = [len(en) for en in en_encoded]\n",
    "kor_len = [len(kor) for kor in kor_encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "827df495-08b0-4616-948f-33615b9eb8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(12,5))\n",
    "\n",
    "axes[0].set_title('Length of english sentence')\n",
    "axes[0].hist(en_len, bins=100, color='r')\n",
    "\n",
    "axes[1].set_title('Length of korean sentence')\n",
    "axes[1].hist(kor_len, bins=100, color='b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e94be-38c0-401e-8afd-c66b5241ea7b",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c7448b2-6fbf-41e6-801d-06255d3b6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderDataset(Dataset) :\n",
    "\n",
    "    def __init__(self, en_list, de_list, sen_size, val_ratio=0.1) :\n",
    "        super(EncoderDecoderDataset , self).__init__()\n",
    "        self.en_in = pad_sequences(en_list, maxlen=sen_size, padding='post')\n",
    "        \n",
    "        de_array = pad_sequences(de_list, maxlen=sen_size+1, padding='post')\n",
    "        self.de_in = de_array[:,:-1]\n",
    "        self.de_out = de_array[:,1:]\n",
    "        \n",
    "        self.val_ratio = val_ratio\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.en_in)\n",
    "\n",
    "    def __getitem__(self , idx) :\n",
    "        en_in_idx = self.en_in[idx]\n",
    "        de_in_idx = self.de_in[idx]\n",
    "        de_out_idx = self.de_out[idx]\n",
    "        \n",
    "        return {'encoder_in' : en_in_idx, 'decoder_in' : de_in_idx, 'decoder_out' : de_out_idx}\n",
    "    \n",
    "    def split_dataset(self) :\n",
    "        n_val = int(len(self) * self.val_ratio)\n",
    "        n_train = len(self) - n_val\n",
    "        train_set, val_set = random_split(self, [n_train, n_val])\n",
    "        \n",
    "        return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97968ac0-37b9-4efa-8d02-15bc5a70b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 25\n",
    "\n",
    "dataset = EncoderDecoderDataset(kor_encoded, en_encoded, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa1de75c-2951-4ebf-9887-a5b3e496cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = dataset.split_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc6ab97a-1554-4ab9-ae89-06b06730970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 60\n",
    "\n",
    "train_loader = DataLoader(train_set ,\n",
    "                          batch_size = batch_size,\n",
    "                          num_workers = 4,\n",
    "                          shuffle = True,\n",
    "                          drop_last = True)\n",
    "\n",
    "val_loader = DataLoader(val_set ,\n",
    "                        batch_size = 100,\n",
    "                        num_workers = 4,\n",
    "                        shuffle = False,\n",
    "                        drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4e0c2-4c37-4022-a513-432666621d0d",
   "metadata": {},
   "source": [
    "## Device & Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "122ff02d-45fe-42f2-8880-c4d08b0629c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "random.seed(20210906)\n",
    "torch.cuda.manual_seed_all(20210906)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed26eb4-4524-4c40-9421-62d25278837d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8385ddf1-9fc9-483a-ab20-fca7d280ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingMask(nn.Module) :\n",
    "\n",
    "    def __init__(self, sen_size) :\n",
    "        super(PaddingMask , self).__init__() \n",
    "        self.sen_size = sen_size\n",
    "    \n",
    "    def forward(self, in_tensor) :\n",
    "        batch_size = in_tensor.shape[0]\n",
    "        # mask tensor which element is 0.0\n",
    "        flag_tensor = torch.where(in_tensor == 0.0 , 1.0 , 0.0)\n",
    "        # shape : (batch_size, 1, 1, sen_size)\n",
    "        flag_tensor = torch.reshape(flag_tensor , (batch_size, 1, 1, self.sen_size)) \n",
    "        \n",
    "        return flag_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00ca3d19-98ab-46c6-aaa3-2404596ac905",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookAheadMask(nn.Module) :\n",
    "\n",
    "    def __init__(self, sen_size, cuda_flag) :\n",
    "        super(LookAheadMask, self).__init__() \n",
    "        self.sen_size = sen_size\n",
    "        self.mask_tensor = self.get_mask(sen_size).cuda() if cuda_flag else self.get_mask(sen_size)\n",
    "\n",
    "    def get_mask(self, sen_size) :\n",
    "        # masking tensor\n",
    "        mask_array = 1 - np.tril(np.ones((sen_size,sen_size)) , 0)\n",
    "        mask_tensor = torch.tensor(mask_array , dtype = torch.float32 , requires_grad=False)\n",
    "        mask_tensor = mask_tensor.unsqueeze(0) # shape : (1, sen_size, sen_size)\n",
    "\n",
    "        return mask_tensor\n",
    "    \n",
    "    def forward(self, in_tensor) :\n",
    "        mask_tensor = torch.maximum(in_tensor, self.mask_tensor)\n",
    "\n",
    "        return mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7de3f039-c5e6-4bd7-a635-f27a6017d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module) :\n",
    "\n",
    "    def __init__(self, pos_len, d_model, cuda_flag) :\n",
    "        super(PositionalEncoding , self).__init__()\n",
    "        self.pos_len = pos_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # w : weight\n",
    "        # pe : Encoding tensor\n",
    "        if cuda_flag == True :\n",
    "            self.w = torch.sqrt(torch.tensor(d_model, dtype=torch.float32, requires_grad=False)).cuda()\n",
    "            self.pe = self.get_embedding(pos_len, d_model).cuda()\n",
    "\n",
    "        else :\n",
    "            self.w = torch.sqrt(torch.tensor(d_model, dtype=torch.float32, requires_grad=False))\n",
    "            self.pe = self.get_embedding(pos_len, d_model)\n",
    "\n",
    "    # Embedding tensor : (batch_size, sen_size, embedding_dimension)\n",
    "    # Making Encoding tensor (1, sen_size, embedding_dimension)\n",
    "    def get_embedding(self, pos_len, d_model) :\n",
    "        pos_vec = torch.arange(pos_len).float()\n",
    "        pos_vec = pos_vec.unsqueeze(1)\n",
    "\n",
    "        i_vec = torch.arange(d_model).float() / 2\n",
    "        i_vec = torch.floor(i_vec) * 2\n",
    "        i_vec = i_vec.unsqueeze(0) / d_model\n",
    "        i_vec = 1 / torch.pow(1e+4 , i_vec)\n",
    "\n",
    "        em = torch.matmul(pos_vec, i_vec)\n",
    "        pe = torch.zeros(pos_len, d_model, requires_grad=False)\n",
    "\n",
    "        sin_em = torch.sin(em)\n",
    "        cos_em = torch.cos(em)\n",
    "\n",
    "        pe[:,::2] = sin_em[:,::2]\n",
    "        pe[:,1::2] = cos_em[:,1::2]\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        return pe\n",
    "\n",
    "    def forward(self, in_tensor) :\n",
    "        en_tensor = in_tensor * self.w + self.pe\n",
    "        \n",
    "        return en_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c5675306-fa27-4b52-9f51-76529149cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module) :\n",
    "\n",
    "    def __init__(self, sen_size,  d_model, num_heads) :\n",
    "        super(MultiHeadAttention , self).__init__()\n",
    "        self.sen_size = sen_size # sen_size\n",
    "        self.d_model = d_model # embedidng_dim\n",
    "        self.num_heads = num_heads # head_size\n",
    "        self.depth = int(d_model / num_heads) # embedding_dim / num_heads\n",
    "\n",
    "        self.q_layer = nn.Linear(d_model , d_model)\n",
    "        self.k_layer = nn.Linear(d_model , d_model)\n",
    "        self.v_layer = nn.Linear(d_model , d_model)\n",
    "        self.o_layer = nn.Linear(d_model , d_model)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.tensor(self.depth , dtype=torch.float32 , requires_grad=False))\n",
    "        \n",
    "        self.init_param()\n",
    "\n",
    "    def split(self, tensor) :\n",
    "        tensor = torch.reshape(tensor , (-1 , self.sen_size , self.num_heads , self.depth)) # (batch_size, sen_size, num_heads, depth)\n",
    "        tensor = torch.transpose(tensor , 2 , 1) # batch_size, num_heads, sen_size, depth)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def merge(self, tensor) :\n",
    "        tensor = torch.transpose(tensor , 2 , 1) # (batch_size, sen_size, num_heads, depth)\n",
    "        tensor = torch.reshape(tensor , (-1 , self.sen_size , self.d_model)) # (batch_size , sen_size , embedding_dim)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def scaled_dot_production(self, q_tensor, k_tensor, v_tensor, m_tensor) :\n",
    "        q_tensor = self.split(q_tensor)\n",
    "        k_tensor = self.split(k_tensor)\n",
    "        v_tensor = self.split(v_tensor)\n",
    "        \n",
    "        k_tensor_T = torch.transpose(k_tensor , 3 , 2) # (batch_size, num_heads, depth, sen_size)\n",
    "\n",
    "        qk_tensor = torch.matmul(q_tensor , k_tensor_T) # (batch_size, num_heads, sen_size, sen_size)\n",
    "        qk_tensor /= self.scale\n",
    "\n",
    "        # pad mask tensor shape : (batch_size, 1, 1, sen_size)\n",
    "        # lookahead mask tensor shape : (batch_size, 1, sen_size, sen_size)\n",
    "        if m_tensor != None :\n",
    "            qk_tensor -= (m_tensor * 1e+6)\n",
    "\n",
    "        qk_tensor = F.softmax(qk_tensor , dim = -1)\n",
    "        att = torch.matmul(qk_tensor , v_tensor) # (batch_size, num_heads, sen_size, depth)\n",
    "\n",
    "        return att\n",
    "\n",
    "    # Xavier Initialization\n",
    "    def init_param(self) :\n",
    "        for m in self.modules() :\n",
    "            if isinstance(m,nn.Linear) :\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, q_in, k_in, v_in, m_in) :\n",
    "        q_tensor = self.q_layer(q_in)\n",
    "        k_tensor = self.k_layer(k_in)\n",
    "        v_tensor = self.v_layer(v_in)\n",
    "\n",
    "        att_tensor = self.scaled_dot_production(q_tensor , k_tensor , v_tensor , m_in)\n",
    "        att_tensor = self.merge(att_tensor)\n",
    "\n",
    "        o_tensor = self.o_layer(att_tensor)\n",
    "\n",
    "        return o_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dea414d7-8b60-4d84-b32d-778f8ab400e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module) :\n",
    "\n",
    "    def __init__(self, hidden_size, d_model) :\n",
    "        super(FeedForward , self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # relu activation and input, output dim are same\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model , hidden_size), \n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(hidden_size , d_model))\n",
    "        self.init_param()\n",
    "                \n",
    "    # He Initialization\n",
    "    def init_param(self) :\n",
    "        gain = 2 ** (1/2)\n",
    "        for m in self.modules() :\n",
    "            if isinstance(m , nn.Linear) :\n",
    "                nn.init.kaiming_normal_(m.weight , gain)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self , in_tensor) :\n",
    "        o_tensor = self.ff(in_tensor)\n",
    "\n",
    "        return o_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd374424-a093-4146-b5ba-60201260dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module) :\n",
    "\n",
    "    def __init__(self, layer_size, sen_size, v_size, d_model, num_heads, hidden_size, \n",
    "                 drop_rate, norm_rate, cuda_flag) :\n",
    "        super(TransformerEncoder , self).__init__()\n",
    "        self.layer_size = layer_size\n",
    "        self.sen_size = sen_size\n",
    "        self.v_size = v_size\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.drop_rate = drop_rate\n",
    "        self.norm_rate = norm_rate\n",
    "\n",
    "        self.em = nn.Embedding(num_embeddings=v_size, embedding_dim=d_model, padding_idx=0) # embedding\n",
    "        self.pos = PositionalEncoding(sen_size, d_model, cuda_flag) # positional encoding\n",
    "        self.pad = PaddingMask(sen_size) # masking\n",
    "        \n",
    "        self.mha_layer = nn.ModuleList()\n",
    "        self.ff_layer = nn.ModuleList()\n",
    "\n",
    "        self.drop_layer = nn.Dropout(drop_rate)\n",
    "        self.norm_layer = nn.LayerNorm(d_model , eps=norm_rate)\n",
    "\n",
    "        for i in range(layer_size) :\n",
    "            self.mha_layer.append(MultiHeadAttention(sen_size , d_model , num_heads))\n",
    "            self.ff_layer.append(FeedForward(hidden_size , d_model))\n",
    "\n",
    "        self.init_param()\n",
    "            \n",
    "    def init_param(self) :\n",
    "        nn.init.normal_(self.em.weight, mean=0.0, std=0.1)\n",
    "            \n",
    "    def forward_block(self, i, in_tensor, m_tensor) :\n",
    "        mha_tensor = self.mha_layer[i](in_tensor , in_tensor , in_tensor , m_tensor)\n",
    "        mha_tensor = self.drop_layer(mha_tensor)\n",
    "        h_tensor = self.norm_layer(in_tensor + mha_tensor)\n",
    "\n",
    "        ff_tensor = self.ff_layer[i](h_tensor)\n",
    "        ff_tensor = self.drop_layer(ff_tensor)\n",
    "        o_tensor = self.norm_layer(h_tensor + ff_tensor)\n",
    "\n",
    "        return o_tensor\n",
    "\n",
    "    def forward(self, in_tensor) :\n",
    "        pad_mask = self.pad(in_tensor)\n",
    "        \n",
    "        em_tensor = self.em(in_tensor)\n",
    "        en_tensor = self.pos(em_tensor)\n",
    "        \n",
    "        tensor_ptr = en_tensor\n",
    "        for i in range(self.layer_size) :\n",
    "            tensor_ptr = self.forward_block(i, tensor_ptr, pad_mask)\n",
    "        \n",
    "        return tensor_ptr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a1db92ae-70b8-4580-83b6-ece97c4257a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module) :\n",
    "\n",
    "    def __init__(self, layer_size, sen_size, v_size, d_model, num_heads, hidden_size, \n",
    "                 drop_rate, norm_rate, cuda_flag) :\n",
    "        super(TransformerDecoder , self).__init__()\n",
    "        self.layer_size = layer_size\n",
    "        self.sen_size = sen_size\n",
    "        self.v_size = v_size\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.drop_rate = drop_rate\n",
    "        self.norm_rate = norm_rate\n",
    "        \n",
    "        self.em = nn.Embedding(num_embeddings=v_size, embedding_dim=d_model, padding_idx=0) # embedding\n",
    "        self.pos = PositionalEncoding(sen_size, d_model, cuda_flag) # positional encoding\n",
    "        self.pad = PaddingMask(sen_size) # padding masking\n",
    "        self.lookahead = LookAheadMask(sen_size , cuda_flag) # lookahead masking\n",
    "        \n",
    "        self.masked_mha_layer = nn.ModuleList()\n",
    "        self.mha_layer = nn.ModuleList()\n",
    "        self.ff_layer = nn.ModuleList()\n",
    "\n",
    "        self.drop_layer = nn.Dropout(drop_rate)\n",
    "        self.norm_layer = nn.LayerNorm(d_model , eps=norm_rate)\n",
    "        \n",
    "        self.o_layer = nn.Linear(d_model, v_size)\n",
    "\n",
    "        for i in range(layer_size) :\n",
    "            self.masked_mha_layer.append(MultiHeadAttention(sen_size , d_model , num_heads))\n",
    "            self.mha_layer.append(MultiHeadAttention(sen_size , d_model , num_heads))\n",
    "            self.ff_layer.append(FeedForward(hidden_size , d_model))\n",
    "    \n",
    "        self.init_param()\n",
    "        \n",
    "    def init_param(self) :\n",
    "        nn.init.normal_(self.em.weight, mean=0.0, std=0.1)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.o_layer.weight, gain=1.0)\n",
    "        nn.init.zeros_(self.o_layer.bias)\n",
    "            \n",
    "    def forward_block(self, i, in_tensor, en_out_tensor, padding_mask, lookahead_mask) :\n",
    "        # query : in_tensor\n",
    "        # key : in_tensor \n",
    "        # value : in_tensor \n",
    "        # mask ; look ahead mask\n",
    "        m_mha_tensor = self.masked_mha_layer[i](in_tensor , in_tensor , in_tensor , lookahead_mask)\n",
    "        m_mha_tensor = self.drop_layer(m_mha_tensor)\n",
    "        h_tensor = self.norm_layer(in_tensor + m_mha_tensor)\n",
    "\n",
    "        # query : output of masked multihead attention\n",
    "        # key : encoder output , \n",
    "        # value : encoder output , \n",
    "        # mask ; pad_tensor of decoder input\n",
    "        mha_tensor = self.mha_layer[i](h_tensor, en_out_tensor, en_out_tensor, padding_mask)\n",
    "        mha_tensor = self.drop_layer(mha_tensor)\n",
    "        a_tensor = self.norm_layer(h_tensor+mha_tensor)\n",
    "\n",
    "        ff_tensor = self.ff_layer[i](a_tensor)\n",
    "        ff_tensor = self.drop_layer(ff_tensor)\n",
    "        o_tensor = self.norm_layer(a_tensor+ff_tensor)\n",
    "\n",
    "        return o_tensor\n",
    "\n",
    "    def forward(self, in_tensor, en_out_tensor) :\n",
    "        pad_mask = self.pad(in_tensor)\n",
    "        lookahead_mask = self.lookahead(pad_mask)\n",
    "        \n",
    "        em_tensor = self.em(in_tensor)\n",
    "        de_tensor = self.pos(em_tensor)\n",
    "        \n",
    "        tensor_ptr = de_tensor\n",
    "        for i in range(self.layer_size) :\n",
    "            tensor_ptr = self.forward_block(i, tensor_ptr, en_out_tensor, pad_mask, lookahead_mask)\n",
    "        o_tensor = self.o_layer(tensor_ptr)\n",
    "        \n",
    "        return o_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41111527-b748-4ea7-8d28-38fc98ea6de1",
   "metadata": {},
   "source": [
    "## Model Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "34e2ebd6-6db0-44b2-8197-88c8a768da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_v_size = en_encoder.get_size()\n",
    "kor_v_size = kor_encoder.get_size()\n",
    "\n",
    "layer_size = 6\n",
    "sen_size = max_len\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "h_size = 2048\n",
    "drop_rate = 1e-1\n",
    "norm_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a32cc66-d3cd-4dda-829d-03ceb495d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(layer_size, sen_size, kor_v_size, \n",
    "                             d_model, num_heads, h_size, drop_rate, norm_rate, use_cuda).to(device)\n",
    "\n",
    "decoder = TransformerDecoder(layer_size, sen_size, en_v_size, \n",
    "                             d_model, num_heads, h_size, drop_rate, norm_rate, use_cuda).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beeb5a7-fcdf-4cf4-9685-351d1ccb1605",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "605b9618-1a19-4fb6-aca1-eebc7ff3392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_lr = 1e-4 # no meaning\n",
    "\n",
    "epoch_size = int(100000 / len(train_loader))\n",
    "warmup_steps = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ff579e67-7775-4655-8ded-a26de77216a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_fn(epoch, d_model, dumb_lr) :\n",
    "    step_num = epoch + 1\n",
    "    val1 = d_model ** (-0.5)\n",
    "    \n",
    "    arg1 = step_num ** (-0.5)\n",
    "    arg2 = (warmup_steps ** (-1.5)) * step_num\n",
    "    \n",
    "    val2 = min(arg1 , arg2) \n",
    "    return (val1 * val2) / dumb_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "57880e66-2a15-473a-b3f0-dac6a4f3c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_param = chain(encoder.parameters(), decoder.parameters())\n",
    "\n",
    "optimizer = optim.Adam(tf_param, lr=dumb_lr, betas=(0.9,0.98), eps=1e-9)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, \n",
    "                                        lr_lambda = lambda epoch: schedule_fn(epoch, d_model, dumb_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd58022f-68c0-48cb-a5aa-0abeee8c04e0",
   "metadata": {},
   "source": [
    "## Acc & Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7674d10b-b778-4e8d-8968-c7e97c397210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_fn(y_output , y_label) :\n",
    "    \n",
    "    y_arg = torch.argmax(y_output, dim=-1)\n",
    "    y_acc = (y_arg == y_label).float()\n",
    "\n",
    "    y_acc = torch.mean(y_acc)\n",
    "    \n",
    "    return y_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "60f934ae-b533-4c39-b58d-76a4275ecee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fb6adb-fc5d-4eaa-a93b-eaf41c3a0796",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "55dc8431-65d7-4f84-bcc7-b0c2a5fede3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('./Log/runs/transformer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0a6849-f33f-455c-8d41-2177c5137e4d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "818615d4-75e6-46a2-bf30-a52a70ac4651",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = np.inf\n",
    "stop_count = 0\n",
    "log_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "14b1a8fe-0ccf-4867-b97b-1484de12e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressLearning(value, endvalue, loss , acc , bar_length=50):\n",
    "    percent = float(value + 1) / endvalue\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}/{2} \\t Loss : {3:.3f} , Acc : {4:.3f}\".format(arrow + spaces, value+1 , endvalue , loss , acc))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6dc38a61-8290-4ce8-a49c-66f2d382e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, test_loader) :\n",
    "    with torch.no_grad() :\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        loss_eval = 0.0\n",
    "        acc_eval = 0.0\n",
    "        for data in test_loader :\n",
    "            en_in = data['encoder_in'].long().to(device)\n",
    "            de_in = data['decoder_in'].long().to(device)\n",
    "            de_label = data['decoder_out'].long().to(device)\n",
    "            \n",
    "            de_label = data['decoder_out'].long().to(device)\n",
    "            de_label = de_label.view([-1,])\n",
    "            \n",
    "            en_output = encoder(en_in)\n",
    "            de_output = decoder(de_in, en_output)\n",
    "            de_output = de_output.view([-1, en_v_size])\n",
    "\n",
    "            loss_eval += loss_fn(de_output , de_label)\n",
    "            acc_eval += acc_fn(de_output , de_label)\n",
    "\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        loss_eval /= len(test_loader)\n",
    "        acc_eval /= len(test_loader)\n",
    "        \n",
    "    return loss_eval , acc_eval  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca61e80-5052-4c67-8e04-aff681fc8374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 \t Learning Rate : 1.746928e-07\n",
      "Percent: [------------------------------------------------->] 1500/1500 \t Loss : 6.397 , Acc : 0.371\n",
      "Val Loss : 6.363 \t Val Accuracy : 0.362\n",
      "\n",
      "Epoch : 1 \t Learning Rate : 3.493856e-07\n",
      "Percent: [------------------------------------------------->] 1500/1500 \t Loss : 5.964 , Acc : 0.346\n",
      "Val Loss : 5.761 \t Val Accuracy : 0.362\n",
      "\n",
      "Epoch : 2 \t Learning Rate : 5.240784e-07\n",
      "Percent: [------------------->                              ] 593/1500 \t Loss : 5.809 , Acc : 0.338"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_size) :\n",
    "    idx = 0\n",
    "    lr_idx = optimizer.param_groups[0]['lr']\n",
    "    print('Epoch : %d \\t Learning Rate : %e' %(epoch , lr_idx))\n",
    "\n",
    "    for data in train_loader :\n",
    "        en_in = data['encoder_in'].long().to(device)\n",
    "        de_in = data['decoder_in'].long().to(device)\n",
    "        \n",
    "        de_label = data['decoder_out'].long().to(device)\n",
    "        de_label = de_label.view([-1,])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        en_output = encoder(en_in)\n",
    "        de_output = decoder(de_in, en_output)\n",
    "        de_output = de_output.view([-1, en_v_size])\n",
    "\n",
    "        loss = loss_fn(de_output , de_label)\n",
    "        acc = acc_fn(de_output , de_label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progressLearning(idx, len(train_loader), loss.item(), acc.item())\n",
    "\n",
    "        if (idx + 1) % 10 == 0 :\n",
    "            writer.add_scalar('train/loss' , loss.item() , log_count)\n",
    "            writer.add_scalar('train/acc' , acc.item() , log_count)\n",
    "            log_count += 1\n",
    "        \n",
    "        idx += 1\n",
    "\n",
    "    val_loss, val_acc = evaluate(encoder, decoder, val_loader)\n",
    "        \n",
    "    writer.add_scalar('test/loss' , val_loss.item() , epoch)\n",
    "    writer.add_scalar('test/acc' , val_acc.item() , epoch)\n",
    "    \n",
    "    if val_loss < min_loss :\n",
    "        min_loss = val_loss\n",
    "        torch.save({'epoch' : (epoch) ,  \n",
    "                    'encoder_state_dict' : encoder.state_dict() , \n",
    "                    'decoder_state_dict' : decoder.state_dict() , \n",
    "                    'loss' : val_loss.item() , \n",
    "                    'acc' : val_acc.item()} , \n",
    "                    f'./Model/checkpoint_transformer.pt')        \n",
    "        stop_count = 0 \n",
    "    \n",
    "    else :\n",
    "        stop_count += 1\n",
    "        if stop_count >= 5 :      \n",
    "            print('\\nTraining Early Stopped')\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    print('\\nVal Loss : %.3f \\t Val Accuracy : %.3f\\n' %(val_loss, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4562ff5-3dbd-48c1-9377-6ea16999c347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d4709d-c685-43bd-90a9-9fa8c03a7ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
